# Generative Adversarial Networks

Link to original paper: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)

## Why and what are Generative Adversarial Networks (GAN)?

Deep learning has resulted in development of discriminative models capable of mapping high-dimensional, rich sensory inputs to a class label. For example, discriminative models can learn to differentiate and label images of animals. The successes can be attributed to algorithms like backpropagation and dropout, often utilising piecewise linear units with well-behaved gradients such as *ReLU*.

In contrast to discriminative models, deep generative models have had less impact. Generative models aim to model the underlying probability distribution of the data, allowing for the generation of new samples that resemble the original dataset. However, deep generative models face several challenges:

1. **Complexity in Probabilistic Computations**: Probabilistic computations involving techniques like maximum likelihood estimation can become intractable (cannot be solved by a polynomial-time algorithm) as the complexity of the mode increases
2. **Difficulty Leveraging Piecewise Linear Units**: It is difficult to leverage piecewise linear units like *ReLU* in generative models. This can be due to reasons such as:
    1. **Smoothness and Non-Smoothness**: Piecewise linear units have regions of smoothness (linearity) and non-smoothness (non-linearity) which might not align well with generative models which often require smooth and continuous representations to model probability distributions
    2. **Model Expressiveness**: Generative models need to capture complex distributions and piecewise linear units might not provide enough expressiveness to capture these complex relationships
    3. **Incorporating Latent Variables**: Generative models often involve latent variables, which are unobserved or hidden variables that capture underlying patterns in data. Piecewise linear units might not effectively capture the complex interactions between observed and latent variables in a way that accurately represents the underlying probability distribution

![Image from [Al Gharakhanian](https://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html)](Images/Untitled.png)

Image from [Al Gharakhanian](https://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html)

In this paper, the authors introduce the concept of **adversarial networks** where a **generative model** is pitted against an adversary: a **discriminative model** that learns to determine whether a sample is produced by the generative model or from the actual data distribution. The analogy used is that the generative model represents a team of counterfeiters trying to produce fake currency and use it without detection, while the discriminative model is like the police, trying to detect the counterfeit currency. Competition results in both teams to improve their methods until generated samples are indistinguishable from data samples. GAN generates samples by passing **random noise** through a multilayer perceptron. This introduces latent variables and allows the generative model to learn features that are robust to variations in the input. Both models are trained using only backpropagation and dropout algorithms, requiring only forward propagation for sampling from the generative model, without the need for approximate inference (approximating complex probability distributions for intractable computations using techniques such as Markov Chain Monte Carlo).

The competition between the two networks is the primary training criterion. GANs operate as a **minimax** game where one player’s gain is the other player’s loss, and the goal is to minimise the maximum possible loss. The objective of the game is to reach a strategic equilibrium, known as a **saddle point**, in their value functions.

Notation

To minimise confusion, here are the notation used throughout this page:

- $x$ : Real data
- $z$ : Latent/ Noise vector
- $G(z)$ : Generated data
- $D(x)$ : Discriminator's evaluation of real data
- $D(G(z))$ : Discriminator's evaluation of generator data

## Adversarial Nets

Both the generator and discriminator are multilayer perceptrons. 
<br />

<u>Generator</u>

The generator is designed to learn a distribution $p_g$ over the data $x$. It starts with random noise vector $z$ drawn from a prior distribution denoted as $p_z(z)$. The generator, represented by $G(z; \theta_g)$ takes in $z$ and generates samples from the data space where $\theta_g$ is the parameters of the multilayer perceptron. 
<br />

<u>Discriminator</u>

The discriminator takes in input $x$ and outputs a single scalar value $D(x)$. This output represents the probability that $x$ comes from the real dataset rather than generated by $G$. 
<br />

<u>Training Criterion</u>

We train $D$ to maximise the probability of assigning the correct label to both training examples and samples from $G$. We train $G$ to minimize $log(1 - D(G(z))$; the $log$ of the probability that $D$ incorrectly identifies the generated data as real data → higher log probability means higher error rate of $D$

<br />

**Derivation of the value function**: the value function is actually derived from the *Binary Cross-Entropy (BCE)* w.r.t. the output of $D$. $G$ tries to maximise the BCE of $D$.

For a single observation $x$, the Binary Cross-Entropy Function is given as:

$$
L = -[ylog(\hat y) + (1-y)log(1-\hat y)]
$$

where $y$ is the ground truth and $\hat y$ is the predicted probability of the model, i.e. .

In the case of GANs, $y$ corresponds to the real/generated label and $\hat y$ corresponds to the probability output of $D$. We have:

1. when $y = 1$ (sample $x$ from real data) → $L = -log[D(x)]$
2. when  $y = 0$ (sample $x$ from $G(z)$) → $L = -log[1 - D(G(z))]$

Combining the two loss terms, the objective function for GANs is given by an expectation over $p_{data}(x)$ and $p_z(z)$

$$
\underset{G}{\operatorname{min}}\space\underset{D}{\operatorname{max}}\space V(G,D) = E_{x \sim p_{data}(x)}[logD(x)] + E_{z \sim p_{z}(z)}[log(1-D(G(z)))]
$$

where $D$ and $G$ play a minimax game with value function $V(G,D)$.

Early in training when $G$ is poor, $D$ can easily identify generated samples as fake, resulting in saturation of the  $log(1 - D(G(z))$ term. To counter this, rather than training $G$ to minimize $log(1 - D(G(z))$, we can train $G$ to maximise $log(D(G(z))$. This encourages $G$ to produce generated samples that the discriminator is more likely to classify as real, providing stronger gradients for $G$ early in learning.
<br />

## Training dynamics and Convergence behaviour of GANs

![Untitled](Images/Untitled%201.png)

Definition

- The **black dotted lines** represents the data generating distribution, $p_{data}$.
- The **<span style="color:green">green solid lines</span>** represent the generative distribution $p_g$.
- The **<span style="color:blue">blue dashed lines</span>** represent the discriminative distribution $D$
- The lower horizontal line is the domain where the random noise $z$ is sampled
- The upper horizontal line is the domain of the data $x$
- The upward arrows show how $G$ imposes $p_g$ to map $x = G(z)$. $G$ contracts in areas where $p_g$ is dense and expands in areas where $p_g$ is sparse.

From (a) to (c), $D$ is trained to discriminate samples from data, converging to $D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$, as signified by the shift in the **<span style="color:blue">blue dashed lines</span>**

This then guides $G(z)$ to flow to regions more likely to be classified as data, signified by the leftwards shift in the **<span style="color:green">green solid lines</span>**

This continues until convergence at (d) where $p_{data} = p_g$ and $D(x) = \frac{1}{2}$(likelihood that $D$ accurately predicts the correct label is half)

**Note**:  $p_{data}$ is discrete and $p_g$ is a learnt continuous representation over the data $x$. Also, $G$ never gets to see the training data, it can only observe it through the behaviour of $D$.
<br />

## Algorithm

![Untitled](Images/Untitled%202.png)

**Notes**:

- For every $k$ updates to $D$, we update $G$ once
- $\nabla_{\theta_g}$ is only dependent on the transformation of $z$ into $G(z)$ so we can discard the $logD(x)$ term as the discriminator’s evaluation of real data $x$ does not contribute to the gradient
- Gradient ascent/descent with momentum utilises past gradient updates to help maintain the direction and speed of optimisation → prevent stagnant learning due to negligible gradients at saddle points
<br />

## Proof of a Global Optimality of $p_g = p_{data}$

**Proposition 1**: For a fixed $G$, the optimal $D$ is

$$
D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}
$$

**Proof**: The goal of $D$, given any $G$, is to maximise the value function $V(G,D)$.

$$
\begin{align}\notag
V(G,D) &= E_{x \sim p_{data}(x)}[logD(x)] + E_{z \sim p_{z}(z)}[log(1-D(G(z)))] \\
&= \notag\int_{x}p_{data}(x)log(D(x))dx + \int_{z}p_{z}(z)log(1-D(G(z)))dz \\
&= \notag\int_{x}p_{data}(x)log(D(x)) + p_{g}(x)log(1-D(x))dx
\end{align}
$$

- **Note**: $x$ in the above equation represents data samples and each term is weighted by the probability of x occurring in $p_{data}$ and $p_g$ respectively (where $x$ belongs to the **black dotted lines** or **<span style="color:green">green solid lines</span>**)

Through a partial derivative of $V(G,D)$ w.r.t. $D(x)$, the optimal discriminator $D^*(x)$ occurs when

$$
\frac{p_{data}(x)}{D(x)} - \frac{p_g(x)}{1-D(x)} = 0
$$

which implies

$$
D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}
$$

concluding the proof of Proposition 1. 
<br />

**Theorem 1**: The global minimum, $\underset{G}{\operatorname{min}}\space V(G,D^*)$, is achieved iff $p_g = p_{data}$. At that point, $\underset{G}{\operatorname{min}}\space V(G,D^*) = -log4$

**Proof**: with Proposition 1, we get

$$
\begin{align}\notag
V(G,D^*) &= E_{x \sim p_{data}(x)}[log(\frac{p_{data}(x)}{p_{data}(x) + p_g(x)})] + E_{x \sim p_{g}(x)}[log(1-\frac{p_{data}(x)}{p_{data}(x) + p_g(x)})] \\
& = E_{x \sim p_{data}(x)}[log(\frac{p_{data}(x)}{p_{data}(x) + p_g(x)})] + E_{x \sim p_{g}(x)}[log(\frac{p_{g}(x)}{p_{data}(x) + p_g(x)})] \notag
\end{align}\notag
$$

By exploiting the properties of logarithms, we get

$$
\begin{align}\notag
V(G,D^*)  & = E_{x \sim p_{data}(x)}[log(\frac{p_{data}(x)}{\frac{p_{data}(x) + p_g(x)}{2}})] + E_{x \sim p_{g}(x)}[log(\frac{p_{g}(x)}{\frac{p_{data}(x) + p_g(x)}{2}})] - 2log2 \\
& = -log4 + E_{x \sim p_{data}(x)}[log(\frac{p_{data}(x)}{\frac{p_{data}(x) + p_g(x)}{2}})] + E_{x \sim p_{g}(x)}[log(\frac{p_{g}(x)}{\frac{p_{data}(x) + p_g(x)}{2}})] \notag
\end{align} \notag
$$

We can transform the expectations in the original function into Kullback-Leibler divergence

$$
V(G,D^*) = -log4 + D_{KL}(p_{data} || \frac{p_{data} + p_g}{2}) + D_{KL}(p_{g} || \frac{p_{data} + p_g}{2})
$$
<details>
<summary>What is Kullback-Leibler divergence</summary>
    
Note that *Kullback-Leibler divergence*, denoted by $D_{KL}(P || Q)$, measures how one probability distribution $P$ diverges from a reference distribution $Q$. It quantifies the difference between the two probability distributions by computing the expected logarithmic difference between $P$ and $Q$ when the data is sample from $P$. In continuous probability distributions, Kullback-Leibler divergence is define by
    
$$
    \begin{align} \notag
    D_{KL}(P || Q)  & = \int_{-\infin}^{\infin}P(x)log(\frac{P(x)}{Q(x)})\space dx \\
    & = E_{x\sim P(x)}[log(\frac{P(x)}{Q(x)})] \notag
    \end{align}
$$
</details>
<br />

Our function can then be represented using **Jensen-Shannon divergence** as such

$$
V(G,D^*) = -log4 + 2JSD(p_{data}||p_g)
$$

<details>
<summary>What is Jensen-Shannon Divergence?</summary>
    
Note that *Jensen-Shannon divergence*, denoted by $JSD(P||Q)$, is based on the Kullback-Leibler divergence and it is a symmetric measurement of the similarity between two probability distributions $P$ and $Q$. It is given by
    
$$
    JSD(P||Q) = JSD(Q||P) = \frac{1}{2}D_{KL}(P||\frac{P+Q}{2}) + \frac{1}{2}D_{KL}(Q||\frac{P+Q}{2})
$$
</details> 
<br />


The Jensen-Shannon divergence between two probability distributions is non-negative and $JSD(p_{data}||p_g) = 0 \rightarrow p_{data} = p_g$

Therefore, in order for $G$ to minimise the value function, the **only** solution is $p_{data} = p_g$. Therefore,

$$
\underset{G}{\operatorname{min}}\space V(G, D^*) = -log4
$$

This proves the global optimality of $p_{data} = p_g$, i.e. $G$ perfectly replicating $p_{data}$

## References

1. [https://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html](https://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html)
2. [https://www.youtube.com/watch?v=eyxmSmjmNS0](https://www.youtube.com/watch?v=eyxmSmjmNS0)
3. [https://www.youtube.com/watch?v=J1aG12dLo4I](https://www.youtube.com/watch?v=J1aG12dLo4I)
4. [https://jaketae.github.io/study/gan-math/](https://jaketae.github.io/study/gan-math/)
5. [https://en.wikipedia.org/wiki/Kullback–Leibler_divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)